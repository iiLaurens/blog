{"pages":[{"url":"/blog/ipython-notebook-aws-salt.html","text":"Problem: How many times have you needed to create a powerful EC2 instance with the the python scientific stack installed and the ipython notebook running? I have to do this at least 2 or 3 times every week. A simple solution is to have an AMI with all the libraries ready and create a new instance every time, you can even have the ipython notebook as an upstart service in ubuntu so it runs when the instance is ready. That was my previous solution, but that was before I learned salt . The problem with the AMI solution is that it gets dated really quickly and while update it is not hard, it is annoying. Also having to login into AWS, look for the AMI and spin up and instance is annoying. Solution: Salt + Anaconda + Vagrant Salt will do the provisioning of the instance using states, that makes the updates of new instances simple as changing a YAML file. Anaconda is the best python scientific distribution I have found and the conda package manager is the perfect solution to install the scientific libraries, I don't want to compile numpy and scipy every time I create the instances, that takes at least 30 minutes. Vagrant is used to create the instance and provision it with salt using one command. Install conda using a salt states is pretty easy because salt has support for pip, and conda is pip installable (you have to run conda init after pip install conda ) so is as simple as: pip-packages : pip.installed : - user : root - names : - conda - require : - pkg : python-dev - pkg : python-pip conda-check : cmd.run : - user : ubuntu - name : \"[ -d /usr/conda-meta ] && echo 'changed=no' || echo 'changed=yes'\" - stateful : True - require : - pip : conda # This will create some files into /usr, needs root conda-init : cmd.wait : - user : root - name : \"conda init\" - watch : - cmd : conda-check The main issue was that salt didn't have support for conda, so I wrote my first salt module to manage conda virtual environments using salt. The code below will create a conda virtual env and install the ipython-notebook, numpy, scipy and pandas libraries using the conda repository also will install luigi (or any other python library you want) using regular pip. All of this will be inside the conda virtual env, because you should use virtual envs. venv : conda.managed : - user : ubuntu - pkgs : ipython-notebook,numpy,scipy,pandas,scikit-learn - require : - cmd : conda-init venv-pip : pip.installed : - user : ubuntu - names : - luigi - bin_env : /home/ubuntu/envs/venv/bin/pip - require : - conda : venv I created another module to start the ipython notebook in the background since salt does not support this natively, the state looks like this: /home/vagrant/nbserver.pid : nbserver.start_server : - ip : 0.0.0.0 - port : 80 - nb_dir : /home/ubuntu/notebooks - require : - conda : venv The final peace of the puzzle is how to spin up the instance quickly, Vagrant with the AWS provider is the solution. Vagrant is mainly used for development, and I use it to develop this solution but it also has a nice integration with AWS, so on the Vagrantfile you only need to change the AWS credentials and the instance configuration. Vagrant . configure ( VAGRANTFILE_API_VERSION ) do | config | # AWS Provider config . vm . provider :aws do | aws , override | aws . access_key_id = \"\" aws . secret_access_key = \"\" aws . keypair_name = \"daniel_keypair\" aws . security_groups = [ \"default\" ] aws . instance_type = \"c3.xlarge\" aws . availability_zone = \"us-east-a\" aws . ami = \"ami-a73264ce\" # Precise 12.04 64 bits override . vm . box = \"dummy\" override . ssh . username = \"ubuntu\" override . ssh . private_key_path = \"~/.ssh/my_keypair.pem\" end How you only need to run vagrant up --provider aws and it will create an instance and provision it with salt. Then just go to the URL of the EC2 instance and the notebook will be running in port 80. NOTE: Be sure to use a security group with port 22 and 80 open There are more things you can configure for your notebook such as passwords and more security, take a look here . Also there are more solutions available such as Continnums Wakari that I personally think is an overkill, I am sure is perfect for some people but not for me. This solution gives you the notebook up and running in two minutes, and if you need more control on the instance you can just SSH to it and do whatever you want. Also you only pay the Amazon fees, so for my personal needs is the perfect solution. The whole code is on github: salt conda module , look in example/ipythonnb","tags":"11","title":"One-liner: Deploy python scipy stack with IPython notebook on AWS"},{"url":"/blog/harvard-ds.html","text":"A few weeks/months ago I found on HackerNews that Harvard was publishing all their Data Science course content (lectures, videos, labs, homeworks) online. I couldn't miss the opportunity to find what are they teaching, it was a nice experience to see how much I have learned and what else I am missing. Also, when I saw that everything was on python I knew they are doing it right! The homeworks are IPython notebooks, the books are what I consider every Data Scientist should read: Python for Data Analysis , Machine Learning for Hackers , Probabilistic Programming and Bayesian methods for Hackers . The homeworks were amazing and useful, I haven't seen all the videos (yet) but the ones I did were amazing. Below I do a little review of each homework, the stuff I learned and stuff that I think should be different. Initially I wanted to use this opportunity to get into Julia , but the amount of libraries that are available on python and not on Julia was to high for me to drive me back into python (once again). Homework 0 My solution Basic introduction to python and some of it scientific libraries. Things I changed: Use beautifulsoup4 instead of 3, not sure why they decided to go with an deprecated version. Homework 1 My solution The main idea was to scrape data from the web, in my opinion one of the most important things a Data Scientist can do is to grab its own data, it gives a big boost to analysis if the person responsible for the analysis decides upfront what data is needed. Is doesn't need to be perfect at first, but usually helps a lot . The final problem shows the properties of bootstrapping, a must know. Also some simple problems to get started with matplotlib. I knew about pattern but never had the chance to play with it. I learned that includes a xml/html parser that creates a DOM so one can access the different fields/tags, I usually use beautifulsoup for html and lxml for xml but is nice to have an alternative. I had no idea about fnmatch , I always used regular expressions but I think that for simple matching tasks I am going to use fnmatch from now own and hopefully will save me some time. Homework 2 My solution A basic introduction to statistical models by doing a predictions the Obama campaign, very interesting exercise. I felt like Nate Silver and it was kind of the idea, show that is possible to do a simple but useful prediction doing a reasonable amount of work. As with a lot of work with data usually the 80% gets done in 20% of the time but the remaining 20% takes the remaining 80% of the time. What I changed: Used scipy.stats for probability calculations instead of doing those myself. DRY. I tried the new ggplot for python to do some of the plots faster, worked perfectly Homework 3 My solution Interesting problem of making a simple prediction of movies reviews intro two categories (fresh or rotten). I liked the problem but I think it would be a good idea to use NLTK instead of hand writing some of the functions and doing a hand made classifier using scikit-learn. I realize that it is really easy to do it using the Vectorizer and Naive Bayes classes and I had never done it before so it was a good learning experience, but an introduction to NLTK would be nice. The example is very easy to reproduce in NLTK as NLTKs author showed in his blog . Is an old post but I was able to reproduce the results a few months ago and I was amazed by the results of a simple sentiment classifier. I believe the course missed an opportunity to teach this. I liked the cross-validation and specially the model calibration sections. Also the rotten tomatoes API is a pretty good source of data. Homework 4 My solution For sure the most interesting homework. Not only a nice collaborative filtering problem (with some bayesian analysis) but also an introduction to MapReduce jobs on hadoop using MRJob. I never used MRJob before so it was nice to use it for the first time but I don't think it will replace my love for luigi . Luigi is a nice and small library form the guys at spotify. It comes with hadoop support built in but at the same time can be extended to any kind of data jobs (e.g. postgres or simple text files). Since I found about luigi I have been writing a lot of code as pipelines and I could not be happier. MRJob killer feature is EMR support that luigi does not have currently. Another nice feature of MRJob is the first class documentation. Luigi on the other hand is limited to a Readme file and you will have to read the source code to discover some features, but it is worth it. I have to say that MRJob is very easy and to MapReduce only jobs is the way to go, I highly recommend it. My knowledge about collaborative filtering was (and still is) very limited so I only have to say that I loved to learn about it! Homework 5 My solution Graphs was always that topic that for some reason I haven't dive into yet, but is definitely in my thing to learn list. So I took this opportunity to learn everything I could by watching the lectures and doing the homework. I also grabbed a copy of graph databases by O'reilly from the university library and I am very exited about it, I am a little behind on my reading so it might take a while. The homework was about the congress and the relation between them. Nothing fancy but interesting. Second homework that is about politics, I guess that the data is available but it would be nice to chose a different topic. Conclusion The objective of learning new stuff was accomplished. The course gave me perfect introduction to a variety of topics and I have more clear ideas on how to proceed from now own. I had a lot of fun doing the homeworks, I learned a lot of stuff I didn't knew before. The statistics sections were a little bit hard for me but that was the idea. I felt really comfortable with a lot of the tools used so I know I am doing it right. Finally, now I know in what I need to focus more. The main conclusion I got is that there is still a lot to learn and the Data Science space keeps changing every day, that is the reason I like it: When I think I am good at something, I realize I am just getting started. Thanks to Harvard for publishing all the content online.","tags":"11","title":"Review: Harvard Data Science - Fall 2013"},{"url":"/blog/nlp-scale-semafor-salt-celery-more.html","text":"This posts describes the implementation of a simple system to parse web pages using SEMAFOR (a SEMantic Analyzer Of Frame Representations) at scale. The system is mainly powered by salt and celery but also uses boto to create worker EC2 instances that parse the documents in parallel and luigi is used to describe the data pipeline in each worker. The whole source can be found on github: danielfrg/semafor-parsing The main idea is the following: We are going to have one master box that has: salt master + celery worker that is going to be waiting for tasks When the master receives a query (list of urls to parse) is going to spin up N number of minions/workers using boto and is going to provision all of them using salt Each minion/worker is going to have SEMAFOR and a celery worker waiting for parsing tasks The master creates a set of parsing tasks based on the number of docs and number of instances Each minion/worker parses the document first using the readability API to get text content from HTML then tokenizing the text into sentences using NLTK and finally parses the sentences using SEMAFOR Each minion/worker uploads the results to S3 The diagram below tries to describe the system. If you dont know what semafor is take a look at the example demo or this is just a basic example: Input: There's a Santa Claus! Output: { \"frames\" :[{ \"target\" :{ \"name\" : \"Existence\" , \"spans\" :[{ \"start\" : 0 , \"end\" : 2 , \"text\" : \"There 's\" }]}, \"annotationSets\" :[{ \"rank\" : 0 , \"score\" : 52.10168633235354 , \"frameElements\" :[{ \"name\" : \"Entity\" , \"spans\" :[{ \"start\" : 2 , \"end\" : 5 , \"text\" : \"a Santa Claus\" }]}]}]}], \"tokens\" :[ \"There\" , \"'s\" , \"a\" , \"Santa\" , \"Claus\" , \"!\" ]} The basic idea is: \"Existing\" is related to \"There 's\" and \"Entity\" is related to \"a Santa Claus\" . How to run this? Very simple, only need to get running the master box. Other options are described in the project README but the easiest way is to use vagrant with the AWS provider, just need to run vagrant up --provider aws in the master directory, this is going to provision the master box. When the box is ready just ssh ( vagrant ssh ) and: Edit ~/semafor/master/app/master/settings.py OR create/edit ~/semafor/master/app/local_settings.py to look like this. S3_PATH = 'WHERE THE SEMAFOR FILES WILL BE UPLOADED' AWS_ACCESS_ID = 'AWS ACCOUNT KEY' AWS_SECRET_KEY = 'AAWS ACCOUNT SECRET' READABILITY_TOKEN = 'READABILITY API TOKEN' SALT_MASTER_PUBLIC_ADDRESS = 'THE IP OF THE MASTER' LUIGI_CENTRAL_SCHEDULER = 'THE IP OF THE LUIGI SCHEDULER, CAN BE THE SAME SALT MASTER' Run celery worker: cd ~/semafor/app/master/ and sh start_worker.sh Everything is ready now! Get some URLS you want to parse and call the celery task semafor_parse(urls, n_instances=1) . A helper script is provided in semafor/master/test.py How it looks This are some screenshots I took while running it: EC2 dashboard when creating 10 instances Log on celery when creating 10 instances Celery log when the instances are provisioned via salt and the celery workers are connected Luigi UI while running Luigi dependency graph, really simple for this case. Discussion I used this opportunity to create a real-life example and keep learning (and loving) salt. At the same time I kept playing with celery and luigi that are 2 libraries that I love mainly because they solve very specific problems and are really easy to use. The system took me a few nights and a weekend to build but I am very happy to the results, it was way easier than I originally though and what makes me even happier is that a few months ago I would consider this a 2/3 month project but I did it in less than a week. Definitely using the right tool for the every step is crucial. I cannot image to provision the EC2 instances in other way that is not salt, the states are not easy to understand at first but they are really powerful. Also not relying on AMIs was a requirement, they are nice in some cases but not for reproducibility. Salt solves this. Celery makes perfect sense when distributing tasks between different servers, and luigi is perfect for developing the data pipeline in every worker: download text + tokenizing + semafor parsing + upload output. Not to mention that develop the distributed tasks are really easy to develop using celery and the data pipeline is super easy to develop using luigi. When building this distributed systems using celery I am always thinking: \"I should do this on hadoop...\" And I love and use hadoop but the reason I did it using other tools is simple: sometimes is just to hard to do it on hadoop... specially when one needs to manage different steps, external tools and intermediate files such as the semafor output. On this particular case one advantage is that semafor is written in Java so it should be \"easy\" to create some Hadoop job on Java, specially with semafor 3 (currently in alpha) that generates a handy .jar with everything in it. But to be honest I have no idea how to write a hadoop job in Java and I have zero interest in learning. I just want to use hadoop as a service, using pig, hive, MRJob or others. I didn't want to mess with Java or the semafor source code I just wanted to use it and get the output, and in that case celery + luigi made it easy to develop. Improvements to be made I am always looking for opportunities to improve and try new tools, this are some thing I would love to do. An alternative to readability: I love the product, but I would love to have the same output offline, I have tried tons of boilerplate removal tools, in various languages (python,java,and more) but the best output is always readability. Django integration with celery so one has an UI to call the tasks. I read that the integration has improved in celery 3.1 Location of the documents: In general should be a good idea to crawl first and the do the semafor parsing. One of my previous posts: Django + Celery + Readability = Python relevant content crawler Location of the semafor output: A better place where analytics can be run easily, maybe a graph database. Neo4j should be easy to get running and integrated, but havn't use it. Progress bars are always nice, a solution as described here and integration with django If you have any other suggestion, improvement or question leave a comment. Or did you ran this on the whole wikipedia and found something interesting? Let me know that to.","tags":"11","title":"NLP at scale: Semafor + salt + celery and more"},{"url":"/blog/month-log.html","text":"I've been busy with real work to write any specific posts for the blog but I realize it was a productive month. I worked a lot and learned quite a few things and new technologies and I have some thoughts about them, I usually use twitter to share simple thoughts but they usually get lost on the noise. So I am going to start a monthly series in which I discuss a little bit about what I have done that month. Hopefully mixing with regular posts I am hoping that this makes me learn more and more stuff every month so I have something to write about. On this case is October and a few weeks of September. Books I read I had to do some pig for my job so I used this opportunity to consolidate a little bit my knowledge. I had worked with pig a little bit before thanks to Coursera Data Science course but this time I went to the source and read Programming Pig by O'reilly. I highly recommend the book to anyone starting with pig but as every technology there is nothing as getting the hands dirty and do real stuff. The book gives you the foundation you need and gets in enough depth so you can write and understand latin pig scripts. I also learned how to run a pig job using a jython UDF on EMR, after some try and error I found the solution and put it on a gist . I found that the people of mortardata.com do the same but with real python so one can use all the libraries available (e.g. NLTK). (FYI they open source their code and is on the new version of pig, 0.12) I cannot recommend enough mortardata.com just create an account and be amazed. Nutch I had to do a lot of crawling this month. The universal solution on this case is Apache Nutch . It was not the most pleasant experience to be honest. It gets the job done? Yes. Do I like it? No. The best part of Nutch is that is \"easy\" to run it on top of Hadoop and distribute the load and maybe I just don't like it that much because I haven't been able to clean all the data as I want and I am blaming Nutch. Another option is scrapy . Vagrant and Saltstack I had finally decided to try vagrant and I love it. I regret all the time that I knew about its existence and didn't read about it. The idea is create a clean virtual box based on a bootstrap so one can know exactly the requirements to run every project. Then is possible deploy it to EC2 using a simple command so the same box will be available online. I was so happy that I could develop and deploy in the same box. It feels clean. Then we have chef or salt . With chef recipes or salt modules one can setup not one virtual local machine but a thousand boxes on the cloud using the same configuration file. The magic occurs when vagrant uses chef recipes or salt modules to create the VM. Development and deployment using the same configuration. Amazing. As a pythonista it was easy to choose salt on top of chef (ruby) but I also read this and I was convinced. I just got started with those technologies but I am very exited and cannot wait to use them more and more to make my work more deployable and production ready. I am also keeping an eye on docker . Luigi Luigi is a small project to create pipelines in python. It doesn't try to solve parallelization: it includes support for Hadoop. It doesn't try to solve task scheduling or distribution: celery exists. Is solves a very specific problem and it was designed with data pipelines in mind. Is a young project but it is developed but guys at spotify and I can see a bright future. Also the pipeline structures makes code very clean, easy to understand and debug. I did a couple of examples and put them on gists: Clean HTML and Index it into Solr Merge files in HDFS and count a json field","tags":"10","title":"Month-Log.oct.2013"},{"url":"/blog/word2vec-yhat.html","text":"A few weeks ago Google released some code to convert words to vectors called word2vec . The company I am currently working on does something similar and I was quite amazed by the performance and accuracy of Google's algorithm so I created a simple python wrapper to call the C code for training and read the training vectors into numpy arrays, you can check it out on pypi (word2vec) . At the same time I found out about yhat , I found about them via twitter and after reading their blog I had to try their product. What they do is very simple but very useful: take some python (scikit-learn) or R classifier and create a REST endpoint to make predictions on new data. The product is still in very beta but the guys were very responsive and they helped me to solve some of my issues. The only restriction I had is the yhat limit for free accounts is 50 Mgs per classifier which on this particular case is not enough so I had to reduce the vector size to 25 from the default (100). And reduce it to only 70k vectors, so the results in the app below are a little limited, but the results are very similar. Training Using my word2vec wrapper is as simple as download and unzip the text8 (link) file and: from word2vec import word2vec word2vec ( 'text8' , 'text8-25.vec' , size = 25 ) This created a file ( text8-25.vec ) with the vectors that can be loaded into numpy. Again using my word2vec wrapper is really simple: from word2vec import WordVectors vectors = WordVectors ( 'text8-31.vec' ) Yhat Then just need to create a yhat model and in the predict method calculate the distance between the vectors. That code is also included on my word2vec package using scipy cosine distance ( example ), on this case I just used the numpy linalg.norm . import numpy as np from yhat import BaseModel class Word2VecCLF ( BaseModel ): def transform ( self , request ): return request def predict ( self , request ): ''' Calculate distances ''' target = request [ 'word' ] n = request [ 'n' ] if 'n' in request else 10 distances = [ 1e6 for i in range ( n )] values = [ None for i in range ( n )] words = self . words vectors = self . vectors target_ix = np . where ( words == target )[ 0 ] for word , vector in zip ( words , vectors ): if word != target : n_dist = np . linalg . norm ( vectors [ target_ix , :] - vector ) if n_dist < max ( distances ): if n_dist < min ( distances ): distances . insert ( 0 , n_dist ) distances = distances [: n ] values . insert ( 0 , word ) values = values [: n ] else : for i , dist_1 in enumerate ( reversed ( distances [: - 1 ])): dist_2 = distances [ n - i - 1 ] if n_dist < dist_2 and n_dist >= dist_1 : distances . insert ( n - i - 1 , n_dist ) distances = distances [: n ] values . insert ( n - i - 1 , word ) values = values [: n ] break return { 'distances' : distances , 'words' : values } Then just need to upload to yhat. from yhat import Yhat yh = Yhat ( \"EMAIL\" , \"TOKEN\" ) yh . deploy ( \"word2vec\" , word2vec_clf ) If everything goes fine you have a REST endpoint you can call. Example I built a simple app using angularJS . Just type any word and the number of close word vectors you want and click the button. On the list that it generates you can click on any word and it will give you the neighbors for that word. Request word distance {{word.word}} {{word.distance}} var app = angular.module('app', []); app.directive('eatClick', function() { return function(scope, element, attrs) { $(element).click(function(event) { event.preventDefault(); }); } }) var MainCtrl = function($scope, $http) { $scope.form_word = ''; $scope.form_n = 10; $scope.words = []; $scope.formRequest = function() { $scope.request($scope.form_word, $scope.form_n); } $scope.listRequest = function(word) { $scope.request(word, $scope.form_n); } $scope.request = function(word, n) { var BASE_URL = 'http://cors.io/api.yhathq.com/predict?username=df.rodriguez143%40gmail.com&model=word2vec&apikey=5162184b820a6ac92274bec2e98b8c88&version=23'; var data = {\"data\": {\"word\": word, \"n\": n} } $http.post(BASE_URL, data) .success(function (data, status, headers, config) { $scope.words = []; for (var i = 0; i < data.prediction.words.length; i++) { $scope.words.push({\"word\": data.prediction.words[i], \"distance\": data.prediction.distances[i]}); } }).error(function (data, status, headers, config) { console.log(data); }); } } Conclusions Definitely some interesting new technologies and tools to keep and eye on. Thanks to Google for open sourcing the code and thanks to yhat for a good product. I had to do something similar a few weeks ago and my solution was to use ZMQ to connect the rest endpoint with the actual classifier yhat makes that possible in 5% of the time. Found some interesting relations? Let me know in the comments below.","tags":"09","title":"word2vec in yhat: Word vector similarity"},{"url":"/blog/django-celery-readability-crawler.html","text":"I have written a few posts about crawling content from the web. Mainly because I know the power of data and the biggest data source in the world is the web, Google knew it and we all now how they are doing. In my last post I wrote about crawling relevant content from blogs; It worked but what if I want an admin UI to control the crawling. What if I just put the crawler in an EC2 instance and just call it when I need to crawl. The solution was pretty simple thanks to some python projects. I just needed to move from sqlalchemy to django ORM, create a few celery tasks and use django-celery to have a pretty UI of the tasks. I was amazed on how easy it was to integrate celery with django. I just created a few tasks to actually crawl blogs (I already had the code from my last post) and create some django actions so the I can create tasks by demand. Tasks The tasks.py is below. Depending on if the blog is wordpress or blogspot I crawl the blog feed differently, that way I can crawl not only the 10 most recent posts but the whole blog. So I created a few tasks to discover the type of the blog. Then based on the type discover the feed URL to crawl. For example wordpress blogs generally have the feed under: http://mywordpressblog.com/feed/ . This gives me only 10 posts if I want more I can request: http://mywordpressblog.com/feed/?paged=2 and that will give me a feed from post 11 to 20. Do this recursively and you get all the posts of the blog. Something similar happens with blogspot. Finally I created some simple tasks to convert the post content to lower-case and word-tokenization using a new but pretty amazing library called TextBlob from celery import task from django.conf import settings from apps.blog_crawler import utils from apps.blog_crawler.models import Blog , Post import time import urllib import readability from bs4 import BeautifulSoup from text.blob import TextBlob @task () def lowerize ( post_id ): post = Post . objects . get ( id = post_id ) post . cleaned = post . cleaned . lower () post . save () @task () def word_tokenize ( post_id ): post = Post . objects . get ( id = post_id ) text = TextBlob ( post . cleaned ) post . cleaned = ' ' . join ( text . words ) post . save () @task () def discover_type ( blog_id ): blog = Blog . objects . get ( id = blog_id ) kind = utils . discover_kind ( blog . url ) blog . kind = kind blog . save () @task () def discover_feed ( blog_id ): blog = Blog . objects . get ( id = blog_id ) if blog . kind is None : kind = utils . discover_kind ( blog . url ) blog . kind = kind feed = utils . discover_feed ( blog . url , blog . kind ) blog . feed = feed blog . save () @task () def crawl ( blog_id , limit = 10 ): blog = Blog . objects . get ( id = blog_id ) # Readability API parser = readability . ParserClient ( settings . READABILITY_PARSER_TOKEN ) # Create and start logger logger = utils . create_logger ( urllib . quote ( blog . url ) . replace ( '/' , '_' )) post_list = utils . get_posts ( blog . feed , blog . kind , limit = limit ) n_posts = len ( post_list ) logger . info ( '{0} ({1})' . format ( blog . url , n_posts )) # Start actual crawl for i , ( url , date ) in enumerate ( post_list ): if len ( Post . objects . filter ( url = url )) > 0 : logger . info ( '{0}/{1} Already exists: {2}' . format ( i , n_posts , url )) else : parser_response = parser . get_article_content ( url ) try : soup = BeautifulSoup ( parser_response . content [ 'content' ]) content = soup . get_text ( ' ' , strip = True ) post = Post ( url = url , content = content , date = date ) post . save () except Exception as e : logger . info ( '{0}/{1} FAIL: {2}' . format ( i + 1 , n_posts , url )) logger . info ( str ( e )) else : logger . info ( '{0}/{1} OK: {2}' . format ( i + 1 , n_posts , url )) time . sleep ( 3.6 ) Django Admin Then just needed to create a django app with some actions to call the celery tasks. This allows me to have the worker on a micro instance on EC2 and just queue the tasks using celery. I used rabbitmq as the broker. from django.contrib import admin from models import Blog , Post from apps.blog_crawler import tasks class BlogAdmin ( admin . ModelAdmin ): list_display = [ 'url' , 'kind' , 'feed' , 'last_crawl' ] ordering = [ 'url' ] actions = [ 'discover_type' , 'discover_feed' , 'crawl' ] def discover_type ( self , request , queryset ): for blog in queryset : tasks . discover_type . delay ( blog . id ) self . message_user ( request , 'Task(s) created' ) def discover_feed ( self , request , queryset ): for blog in queryset : tasks . discover_feed . delay ( blog . id ) self . message_user ( request , 'Task(s) created' ) def crawl ( self , request , queryset ): for blog in queryset : tasks . crawl . delay ( blog . id ) self . message_user ( request , 'Task(s) created' ) discover_type . short_description = 'Discover the type of the blog(s)' discover_feed . short_description = 'Discover the feed of the blog(s)' crawl . short_description = 'Crawls the selected blog(s)' class PostAdmin ( admin . ModelAdmin ): list_display = [ 'url' , 'date' ] actions = [ 'copy' , 'word_tokenize' , 'lowerize' ] def copy ( self , request , queryset ): for post in queryset : post . cleaned = post . content post . save () self . message_user ( request , 'Content copied' ) def word_tokenize ( self , request , queryset ): for post in queryset : tasks . word_tokenize . delay ( post . id ) self . message_user ( request , 'Task(s) created' ) def lowerize ( self , request , queryset ): for post in queryset : tasks . lowerize . delay ( post . id ) self . message_user ( request , 'Task(s) created' ) copy . short_description = 'Copy the crawled content to cleaned' word_tokenize . short_description = 'Tockenize by words' lowerize . short_description = 'Lower-case the cleaned content' admin . site . register ( Blog , BlogAdmin ) admin . site . register ( Post , PostAdmin ) UI The admin UI, using the beautiful django suit , looks like this. Note: don't ask me why is I am using that blog to test You can see the running tasks, or previous tasks. Conclusion I was pretty happy with this result. With very little effort I was able to build a quite complex system. PostgreSQL for the database, rabbitmq + celery for message/tasks queuing and finally Django for the UI. I can even create some I consider this an example of why I choose python and its amazing community. The people working on these projects are brilliant and are doing an amazing job. Focusing on building the basic tools so people can build amazing stuff faster and easier than ever. Some stuff I want to try next if real live allows me to is: Haystack + Solr for search MongoDB instead of Postgres. Apparently is really easy using django-nonrel As usual everything is on github: django-crawler","tags":"09","title":"Django + Celery + Readability = Python relevant content crawler"},{"url":"/blog/nba-scraping-data.html","text":"I've been wanting to play with some sports data for a while. Today I decide to stop procastinating and do it. The problems was that after searching a while (15 minutes) for some data I was unable to find the data I wanted. Even in the Basktetball Database (not really sure I undestand the site). A friend showed me the ESPN stats and ask me if I knew how to scrap the data from a website. I lied and told him Yes. But I know python and its magic powers so after reading 15 minutes I knew how to do it. I used requests and beautifulsoup to download and scrap the data from the ESPN site. Then used pandas to order, slice, and save the data into simple csv files. Also used iPython notebooks to develop the code faster. And a little bit of my copper project to use the data analysis project structure. Get the teams First I needed to get all the team names and URLs. So is as simple as request http://espn.go.com/nba/teams and 32 lines of python. import copper import pandas as pd import requests from bs4 import BeautifulSoup copper . project . path = '../../' url = 'http://espn.go.com/nba/teams' r = requests . get ( url ) soup = BeautifulSoup ( r . text ) tables = soup . find_all ( 'ul' , class_ = 'medium-logos' ) teams = [] prefix_1 = [] prefix_2 = [] teams_urls = [] for table in tables : lis = table . find_all ( 'li' ) for li in lis : info = li . h5 . a teams . append ( info . text ) url = info [ 'href' ] teams_urls . append ( url ) prefix_1 . append ( url . split ( '/' )[ - 2 ]) prefix_2 . append ( url . split ( '/' )[ - 1 ]) dic = { 'url' : teams_urls , 'prefix_2' : prefix_2 , 'prefix_1' : prefix_1 } teams = pd . DataFrame ( dic , index = teams ) teams . index . name = 'team' print ( teams ) copper . save ( teams , 'teams' ) This saves teams.csv file with the 30 teams in this format. team,prefix_1,prefix_2,url Boston Celtics,bos,boston-celtics,http://espn.go.com/nba/team/_/name/bos/boston-celtics Brooklyn Nets,bkn,brooklyn-nets,http://espn.go.com/nba/team/_/name/bkn/brooklyn-nets ...... Get games Then I needed the games information. For this is necessary to read the previous csv file and for each team make a request and parse the data. On this case 60 lines of python produced a 1084 rows csv file with all the games of the current (2013) season. But is as simple as changing a variable to download other seasons information. import copper import numpy as np import pandas as pd import requests from bs4 import BeautifulSoup from datetime import datetime , date copper . project . path = '../../' year = 2013 teams = copper . read_csv ( 'teams.csv' ) BASE_URL = 'http://espn.go.com/nba/team/schedule/_/name/{0}/year/{1}/{2}' match_id = [] dates = [] home_team = [] home_team_score = [] visit_team = [] visit_team_score = [] for index , row in teams . iterrows (): _team , url = row [ 'team' ], row [ 'url' ] r = requests . get ( BASE_URL . format ( row [ 'prefix_1' ], year , row [ 'prefix_2' ])) table = BeautifulSoup ( r . text ) . table for row in table . find_all ( 'tr' )[ 1 :]: # Remove header columns = row . find_all ( 'td' ) try : _home = True if columns [ 1 ] . li . text == 'vs' else False _other_team = columns [ 1 ] . find_all ( 'a' )[ 1 ] . text _score = columns [ 2 ] . a . text . split ( ' ' )[ 0 ] . split ( '-' ) _won = True if columns [ 2 ] . span . text == 'W' else False match_id . append ( columns [ 2 ] . a [ 'href' ] . split ( '?id=' )[ 1 ]) home_team . append ( _team if _home else _other_team ) visit_team . append ( _team if not _home else _other_team ) d = datetime . strptime ( columns [ 0 ] . text , '%a, %b %d ' ) dates . append ( date ( year , d . month , d . day )) if _home : if _won : home_team_score . append ( _score [ 0 ]) visit_team_score . append ( _score [ 1 ]) else : home_team_score . append ( _score [ 1 ]) visit_team_score . append ( _score [ 0 ]) else : if _won : home_team_score . append ( _score [ 1 ]) visit_team_score . append ( _score [ 0 ]) else : home_team_score . append ( _score [ 0 ]) visit_team_score . append ( _score [ 1 ]) except Exception as e : pass # Not all columns row are a match, is OK # print(e) dic = { 'id' : match_id , 'date' : dates , 'home_team' : home_team , 'visit_team' : visit_team , 'home_team_score' : home_team_score , 'visit_team_score' : visit_team_score } games = pd . DataFrame ( dic ) . drop_duplicates ( cols = 'id' ) . set_index ( 'id' ) print ( games ) copper . save ( games , 'games' ) Get player stats All the previous data is good but for doing some analysis I need players stats. Then for each game I download all players stats for that game. The result was 55 lines of python (imports included) to generate 27645 rows full with stats. import copper import numpy as np import pandas as pd import requests from bs4 import BeautifulSoup from datetime import datetime , date copper . project . path = '../..' games = copper . read_csv ( 'games.csv' ) . set_index ( 'id' ) BASE_URL = 'http://espn.go.com/nba/boxscore?gameId={0}' request = requests . get ( BASE_URL . format ( games . index [ 0 ])) table = BeautifulSoup ( request . text ) . find ( 'table' , class_ = 'mod-data' ) heads = table . find_all ( 'thead' ) headers = heads [ 0 ] . find_all ( 'tr' )[ 1 ] . find_all ( 'th' )[ 1 :] headers = [ th . text for th in headers ] columns = [ 'id' , 'team' , 'player' ] + headers players = pd . DataFrame ( columns = columns ) def get_players ( players , team_name ): array = np . zeros (( len ( players ), len ( headers ) + 1 ), dtype = object ) array [:] = np . nan for i , player in enumerate ( players ): cols = player . find_all ( 'td' ) array [ i , 0 ] = cols [ 0 ] . text . split ( ',' )[ 0 ] for j in range ( 1 , len ( headers ) + 1 ): if not cols [ 1 ] . text . startswith ( 'DNP' ): array [ i , j ] = cols [ j ] . text frame = pd . DataFrame ( columns = columns ) for x in array : line = np . concatenate (([ index , team_name ], x )) . reshape ( 1 , len ( columns )) new = pd . DataFrame ( line , columns = frame . columns ) frame = frame . append ( new ) return frame for index , row in games . iterrows (): print ( index ) request = requests . get ( BASE_URL . format ( index )) table = BeautifulSoup ( request . text ) . find ( 'table' , class_ = 'mod-data' ) heads = table . find_all ( 'thead' ) bodies = table . find_all ( 'tbody' ) team_1 = heads [ 0 ] . th . text team_1_players = bodies [ 0 ] . find_all ( 'tr' ) + bodies [ 1 ] . find_all ( 'tr' ) team_1_players = get_players ( team_1_players , team_1 ) players = players . append ( team_1_players ) team_2 = heads [ 3 ] . th . text team_2_players = bodies [ 3 ] . find_all ( 'tr' ) + bodies [ 4 ] . find_all ( 'tr' ) team_2_players = get_players ( team_2_players , team_2 ) players = players . append ( team_2_players ) players = players . set_index ( 'id' ) print ( players ) copper . save ( players , 'players' ) The file looks like this ,id,team,player,MIN,FGM-A,3PM-A,FTM-A,OREB,DREB,REB,AST,STL,BLK,TO,PF,+/-,PTS 0,400277722,Boston Celtics,Brandon Bass,28,6-11,0-0,3-4,6,5,11,1,0,0,1,2,-8,15 0,400277722,Boston Celtics,Paul Pierce,41,6-15,2-4,9-9,0,5,5,5,2,0,0,3,-17,23 ... 0,400277722,Miami Heat,Shane Battier,29,2-4,2-3,0-0,0,2,2,1,1,0,0,3,+12,6 0,400277722,Miami Heat,LeBron James,29,10-16,2-4,4-5,1,9,10,3,2,0,0,2,+12,26 0,400277722,Miami Heat,Chris Bosh,37,8-15,0-1,3-4,2,8,10,1,0,3,1,3,+15,19 ..... A LOT OF DATA ..... Conclusion I love python more. Still a lot of work is needed in order to make sense of all that data. But at least now I have some data. Next step is probably to insert the data into a postgres database. Or just be crazy and do some machine learning as it is. The data and code is on github nba .","tags":"04","title":"Extracting NBA data from ESPN"},{"url":"/blog/pelican-ipython-notebook-plugin.html","text":"This is just a little update on my previous post about Blogging with iPython notebooks with pelican . One of the people behind pelican helped me to convert the previous code to a pelican plugin and I just made it available via github: pelican-ipythonnb . The only thing that changed is the installation but the docs on how to use it is below (or in the readme of the repo). An example is my last post about a cleaning data for a Kaggle competition . Happy blogging! Installation Download plugin files: plugin/ipythonnb.py and the plugin/nbconverter directory. The esiest way is to locate the pelican directory (for example: ~/.virtualenvs/blog/lib/python2.7/site-packages/pelican/ ) and paste plugins files in the pelican/plugins folder. Then in the pelicanconf.py put: PLUGINS = ['pelican.plugins.ipythonnb'] . But is also is possible to add plugins on the same directory of the pelican project: Create a folder called custom_plugins_dir (NOTE: the name can be anything but do not use plugins ) and paste the plugin files there. Then in the pelicanconf.py put: PLUGINS = ['custom_plugins_dir.ipythonnb'] . In both cases also need to modify the MARKUP setting: On the pelicanconf.py put: MARKUP = ('md', 'ipynb') Add the CSS to the theme Download the ipython.css file from the assets directory and place it in your theme static folder. Then include the CSS on the theme template: {% if article.ipython %} <link rel=\"stylesheet\" href=\"/theme/css/ipython.min.css\"> {% endif %} How to blog Write the post using the iPython notebook interface, using markdown or anything. Then open the .ipynb file in a text editor and should see. { \"metadata\": { \"name\": \"Super iPython NB\" }, { A_LOT_OF_OTHER_STUFF } Add the metadata for example: { \"metadata\": { \"name\": \"Super iPython NB\", \"Title\": \"Blogging with iPython notebooks in pelican\", \"Date\": \"2013-2-16\", \"Category\": \"Category\", \"Tags\": \"tag2, tag2\", \"slug\": \"slug-slug-slug\", \"Author\": \"Me\" }, { A_LOT_OF_OTHER_STUFF } And thats it! Add the .ipynb file to the content folder in the pelican project and should generate a new post.","tags":"03","title":"Plugin for blogging with IPython notebooks in Pelican"},{"url":"/blog/blogging-pelican-ipython-notebook.html","text":"Update: Check out the updated post on blogging with iPython notebook and pelican with a plugin . It seems that I spend most time redesigning/developing this blog than actually blogging xD, just a few weeks ago I wrote about how I was blogging using jekyll with iPython notebooks and now I am talking about doing the same different static blog engine. The fact is that a few days ago I found pelican , the first serious python alternative to Jekyll I have found, and after reading 15 minutes about it I was downloading pelican and creating a theme based on my old Jekyll site. The switch was easy just needed move some files, change some of the tags from liquid to jinja2 and add some metadata. The fact of having less than 10 posts also helped because I did that part manually. The fact that pelican is on python gave me the idea of creating an automated way of blogging with ipython notebooks, so I dive a little bit into the pelican documentation (which is great) and decide to make it. How I did it Pelican uses what thet call readers to read different formats (markdown, rst, and now ipynb) and output general HTML so it is very easy to implement a markup for any language. Also the iPython team has already a very good set of converts so all I did was to copy and integrate that necessary code. Finally I took some of the code from Jake Vanderplas when he talks about blogging with Octopress and iPython . The idea is to encapsulate the ipython notebook HTML and CSS so the CSS of the site don't get messed up. How to use it I made a pull request on the official pelican repo but that may take some time so if if you are impatient can use my forked version (brach: ipythonnb-reader) or even just made the changes yourself as are just a few. Before anything install pelican and ipython with pip install pelican ipython also markdown if you want First, locate the pelican folder: mine is: ~/.virtualenvs/blog/lib/python2.7/site-packages/pelican/ At beginning of readers.py add this try : import json import IPython from datetime import datetime from util.nbconverter.html import ConverterHTML except : IPython = False On readers.py just before _EXTENSIONS = {} paste the main reader code class IPythonReader ( Reader ): enabled = bool ( IPython ) file_extensions = [ 'ipynb' ] def read ( self , filename ): text = open ( filename ) converter = ConverterHTML ( filename ) converter . read () metadata_uni = json . load ( text )[ 'metadata' ] metadata2 = {} # Change unicode encoding to utf-8 for key , value in metadata_uni . iteritems (): if isinstance ( key , unicode ): key = key . encode ( 'utf-8' ) if isinstance ( value , unicode ): value = value . encode ( 'utf-8' ) metadata2 [ key ] = value metadata = {} for key , value in metadata2 . iteritems (): key = key . lower () metadata [ key ] = self . process_metadata ( key , value ) metadata [ 'ipython' ] = True content = converter . main_body () # Use the ipynb converter # change ipython css classes so it does not mess up the blog css content = ' \\n ' . join ( converter . main_body ()) # replace the highlight tags content = content . replace ( 'class=\"highlight\"' , 'class=\"highlight-ipynb\"' ) # specify <pre> tags content = content . replace ( '<pre' , '<pre class=\"ipynb\"' ) # create a special div for notebook content = '<div class=\"ipynb\">' + content + \"</div>\" # Modify max-width for tables content = content . replace ( 'max-width:1500px;' , 'max-width:650px;' ) # h1,h2,... for h in '123456' : content = content . replace ( '<h %s ' % h , '<h %s class=\"ipynb\"' % h ) return content , metadata Second, Create a new folder called util on the pelican directory and then download the contents of my branch to that folder. Now you are ready to blog with the iPython notebooks. How to blog First, need to update the MARKUP setting on pelicanconf.py to: MARKUP = ('md', 'ipynb') that way pelican recognizes the ipynb format as an article. Create you super amazing iPython notebook with charts, markdown, etc and save it to the content folder of your pelican site. Open the .ipynb file on your favorite text editor and should see something like this: { \"metadata\" : { \"name\" : \"Super iPython NB\" }, { A_LOT_OF_OTHER_STUFF } Modify the file to: { \"metadata\" : { \"name\" : \"Super iPython NB\" , \"Title\" : \"Blogging with iPython notebooks in pelican\" , \"Date\" : \"2013-2-16\" , \"Category\" : \"Category\" , \"Tags\" : \"tag2, tag2\" , \"slug\" : \"slug-slug-slug\" , \"Author\" : \"Me\" }, { A_LOT_OF_OTHER_STUFF } Note that only the title and date tags are actually necessary. Thats it, couldn't be more simple! One final thing to do only once is to include the the ipython.min.css in your pelican theme. Since not all posts are going to be in iPython is not necessary to include the css file on all pages; so to be more efficient I created a metadata to identify which posts are from ipython notebooks so you in your pelican article.html template should do: {% if article.ipython %} < link rel = \"stylesheet\" href = \"/theme/css/ipython.min.css\" > {% endif %} This particular post uses markdown but most of my previous posts are now converted from ipython notebooks for example: copper-quick-data-transformation If are curious about my pelican config or theme check it out on github . Happy blogging! Update: Check out the updated post on blogging with iPython notebook and pelican with a plugin .","tags":"02","title":"Blogging with IPython notebooks in pelican"},{"url":"/blog/d3-backbone-tornado-histogram-csv.html","text":"After being procrastinating for weeks the learning of D3.js and backbone.js I have finally made my first example using both libraries to explore (via histograms) a pandas DataFrame. The reason of the procrastination is very simple: I love python to much, because is probably the only language who is great in all areas (that I am interested at least): Great web frameworks as Django and Tornado - \"fighting\" with ruby ( rails ) Great Data Analysis packages such as pandas - \"fighting\" with R Great machine-learning libraries such as scikit-learn Probably not the most successful but has a good gaming library pyGame Is a great general purpose language - I use it to program a robot for a NASA competition using a PS3 controller, serial-ports, web-server, cameras, and all in one language And the list could go for hours For me that is the python killer feature: do anything on one language, sometimes the community is not big enough and is necessary the help of a genius like Wes Mckinney to create pandas but in general the community is great. The Javascript community is in a similar condition everyday new libraries come up and change how people do stuff, such as D3.js changed how people do interactive visualizations. So I finally got out of my python comfort zone and re-learn Javascript, I had used it but was very simple examples using JQuery and a very long time ago, everything has changed now the cool people use Backbone.js which is a fabulous creation to give structure to Javascript. So I read a lot of beginner tutorials of backbone and D3 and this is the result. What I wanted For copper , my python data analysis library, I use matplotlib to plot histograms like this: The histograms work and are great so I wanted to do the same with D3.js to explore a csv file using pandas. Data - I am using data from a Business Intelligence project I am currently working on, available here: expedia.csv Python: REST API To use backbone is necessary to have a RESTful API, I select Tornado and pandas to do that I was tempted to use node but at the end of the day I want to keep adding features to copper . The API is the following: /columns returns a list of the columns of the csv file /hist/{id} return necessary data of the columns id to create the histogram Some relevant code: class HistogramHandler ( tornado . web . RequestHandler ): def get ( self , col_id ): df = pd . read_csv ( 'explore.csv' ) col = df [ df . columns [ int ( col_id )]] nans = int ( len ( col ) - col . count ()) col = col . dropna () ans = {} ans [ 'col_name' ] = col . name ans [ 'nans' ] = nans ans [ 'values' ] = col . values . tolist () self . write ( json . dumps ( ans )) application = tornado . web . Application ([ ( r\"/\" , MainHandler ), ( r\"/hist/([0-9]+)\" , HistogramHandler ), ( r\"/columns\" , ColumnsHandler ), ], debug = True , ** settings ) calling /columns returns this: [{ \"id\" : 0 , \"name\" : \"x1\" }, { \"id\" : 1 , \"name\" : \"x2\" }, ... , { \"id\" : 39 , \"name\" : \"x40\" }, { \"id\" : 40 , \"name\" : \"depend\" }] calling /hist/2 returns this: { \"col_name\" : \"x3\" , \"values\" : [ 3.0 , 4.0 , 4.0 , ... , 2.0 , 4.0 ], \"nans\" : 102 } Javascript: UI + Graphics For backbone I mainly read this article on cascading selects . But I use only one select (for now) and use the underscore templates. To make the <select>'s good better I am using a js library called chosen which is really good and simple to use. For D3 I follow the main histogram example and added a red column to show missing values and hard-code it a little bit more to make changes easier. Finally I create another Model and View to control the number of bins dynamically. The complete Javascript including backbone and D3 code is this: $ ( function (){ var ColumnListItem = Backbone . Model . extend (); var ColumnList = Backbone . Collection . extend ({ url : '/columns' , model : ColumnListItem }); var ColumnListView = Backbone . View . extend ({ initialize : function () { _ . bindAll ( this , 'addOne' , 'addAll' ); this . collection . bind ( 'reset' , this . addAll ); }, addOne : function ( column ) { // column is a ColumnListItem model var variables = { id : column . get ( 'id' ), name : column . get ( 'name' ) }; var template = _ . template ( $ ( \"#option_template\" ). html (), variables ); $ ( this . el ). append ( template ); }, addAll : function () { this . collection . each ( this . addOne ); // For each item in the collection call addOne $ ( \".chzn-select\" ). chosen (); }, events : { \"change\" : \"changeSelectedItem\" }, changeSelectedItem : function ( evt ) { histogramData . set ( 'id' , $ ( this . el ). val ()) histogramData . fetch () } }) var NumberBins = Backbone . Model . extend ({ defaults : { bins : 20 , } }); var NumberBinsView = Backbone . View . extend ({ events : { \"change\" : \"changeValue\" }, changeValue : function ( event ){ this . model . set ( 'bins' , parseInt ( this . $el . val ())); } }); var HistogramData = Backbone . Model . extend ({ urlRoot : '/hist' , }); var ChartView = Backbone . View . extend ({ initialize : function () { this . model . get ( 'histogramData' ). bind ( 'sync' , this . render , this ); this . model . get ( 'numberBins' ). bind ( 'change' , this . render , this ); }, render : function () { this . $el . html ( '' ); var histData = this . model . get ( 'histogramData' ). toJSON (); if ( histData . id == undefined ) { return this ; } var nbins = this . model . get ( 'numberBins' ). toJSON (). bins ; var width = 960 ; var height = 500 ; var padding = { top : 15 , right : 15 , bottom : 25 , left : 30 }; var values = histData . values ; var data = d3 . layout . histogram () . bins ( nbins ) ( values ); var xmin = d3 . min ( data , function ( d ) { return d . x ; }); var xmax = d3 . max ( data , function ( d ) { return d . x + d . dx ; }); var ymax = d3 . max ( data , function ( d ) { return d . y ; }); ymax = Math . max ( histData . nans , ymax ); var xScale = d3 . scale . linear () . domain ([ xmin , xmax ]) . range ([ padding . left , width - padding . right ]); var yScale = d3 . scale . linear () . domain ([ 0 , ymax ]) . range ([ height - padding . bottom , padding . top ]); var barWidth = xScale ( data [ 1 ]. x ) - xScale ( data [ 0 ]. x ) - 1 var svg = d3 . select ( this . el ) . append ( \"svg\" ) . attr ( \"width\" , width ) . attr ( \"height\" , height ) if ( histData . nans > 0 ){ xScale = d3 . scale . linear () . domain ([ xmin , xmax ]) . range ([ padding . left + barWidth , width - padding . right ]); barWidth = ( xScale ( data [ 1 ]. x ) - xScale ( data [ 0 ]. x )) - 1 yNans = histData . nans svg . append ( \"g\" ) . attr ( \"class\" , \"bar-nans\" ) . attr ( \"transform\" , function ( d ) { return \"translate(\" + ( padding . left + 2 ) + \",\" + yScale ( yNans ) + \")\" ; }) . append ( \"rect\" ) . attr ( \"width\" , barWidth - 1 ) . attr ( \"height\" , height - yScale ( yNans ) - padding . bottom ); } var bar = svg . selectAll ( \".bar\" ) . data ( data ) . enter (). append ( \"g\" ) . attr ( \"class\" , \"bar\" ) . attr ( \"transform\" , function ( d ) { return \"translate(\" + xScale ( d . x ) + \", \" + yScale ( d . y ) + \")\" }); bar . append ( \"rect\" ) . attr ( \"class\" , \"hint hint--top\" ) . attr ( \"data-hint\" , \"hober me\" ) . attr ( \"data-placement\" , \"top\" ) . attr ( \"data-content\" , \"Vivamus sagittis lacus vel augue laoreet rutrum faucibus.\" ) . attr ( \"width\" , barWidth ) . attr ( \"height\" , function ( d ) { return height - yScale ( d . y ) - padding . bottom ; }); // Axis var xAxis = d3 . svg . axis () . scale ( xScale ) . orient ( \"bottom\" ); var yAxis = d3 . svg . axis () . scale ( yScale ) . orient ( \"left\" ) . ticks ( 5 ); svg . append ( \"g\" ) . attr ( \"class\" , \"x axis\" ) . attr ( \"transform\" , \"translate(0,\" + ( height - padding . bottom ) + \")\" ) . call ( xAxis ); svg . append ( \"g\" ) . attr ( \"class\" , \"y axis\" ) . attr ( \"transform\" , \"translate(\" + padding . left + \",0)\" ) . call ( yAxis ); return this ; }, }) var ChartModel = Backbone . Model . extend (); var columnsList = new ColumnList (); var columnsListView = new ColumnListView ({ el : $ ( \"#column-select\" ), collection : columnsList }); var numberBins = new NumberBins (); var numberBinsView = new NumberBinsView ({ el : $ ( \"#num-bins\" ), model : numberBins }); var histogramData = new HistogramData (); var chartModel = new ChartModel ({ histogramData : histogramData , numberBins : numberBins }); var chartView = new ChartView ({ el : '#chart' , model : chartModel }); columnsList . fetch (); }) Conclusion Changing the number of bins or the column automaticlly re-draws the histogram. It was hard for me to use javascript again. Learning backbone was even harder but I think I am finally understanding it, definitely the best way is to learn it is to take baby steps and read a lot. D3 wasn't that hard, is a very good library. I include the whole code on copper (my data analysis package) which is not yet available via pip, so to use this need to download the explore folder of the package and run python explore.py (requires tornado and pandas). Remember to change the explore.csv to explore other data.","tags":"02","title":"Using D3, backbone and tornado to visualize histograms of a csv file"},{"url":"/blog/new-blog-using-jekyll-and-ipython.html","text":"Recently I restart my blog by moving from wordpress.com to github pages the reason is than about two weeks ago I found a blog on a github page using octopress ( jekyll ) and after reading 20 minutes I immediately wanted to use it and decide to switch. Wordpress is a good piece of software but wordpress.com is not, my complaints are. Is not fast enough Can't have custom analytics and the wordpress analytics sucks. Can't modify HTML or even CSS Don't have markdown support Jekyll has all of that and nd github pages for hosting is just magnificent. Publish a new post with git is how a hacker should do it. I tried to find a python alternative, there are some specially Hyde but the github page shows that the last commit was 2 years ago, while jekyll is actively developed and a much better alternative. At the end of the day it is not necessary to use ruby which is good because I have no idea about it. And the most interesting part is that it adapts to any person/environment. For example can use only markdown but my past three posts have been done with iPython notebook and then converted to html. Which is just an amazing way to share, and at least 10 times faster than copy/pasting code to Wordpress. So i decide to design my own blog using jekyll. I discard octopress because I wanted to be a more customized site and really is not necessary. Design I am not a designer so I use Bootstrap and take inspiration from some site (that is copy a lot of CSS). I tried to learn haml but did not work for me, instead I prefer the zen-coding plugin. Finally I use Less with a Jekyll plugin to do the CSS. iPython notebook I use a little modified version of this code to convert an iPython notebook to HTML and added it to Jekyll without messing the CSS of the blog: Blogging With IPython in Octopress The only modification I did was to merge two commands into one, because I am that lazy. Plugins Github pages support Jekyll but not some of its plugins such as the LESS plugins. There is a very simple workaround for that. Just have to run Jekyll locally with all the plugins you want, and then just upload the compiled HTML,CSS,JS,... to the repository and github will display the static/compiled content. So in my repository I have all the templates, posts, images, markdown, and everything else on folder named source ; that way jekyll creates the static content into source/_site . Then when I am happy with new post or new design I use a very simple python fabfile to clean the root (ignoring the source and .git folders) then it moves the contents of source/_site to the root of the repository, then commits the changes and finally clean the root again. I try to make the site plugin agnostic because I don't now ruby but I try to complement that with a few YAML tags and if's on the layous, for example only have to put a YAML tag on each post file and the picture is added automaticly to the index and post pages. That also happens with youtube and vimeo videos. Finally the source of the site is on github if anyone want to take a look.","tags":"01","title":"New blog using Jekyll & iPython"},{"url":"/blog/python-replace-sas.html","text":"Why? The last fall (2012) I took a class called Business Intelligence (at UT Dallas) which is the same as Data Mining or Machine Learning to business people. On that class they taught us to use SAS Enterprise Miner. I used it but I hate to use it. First of all I had to pay for it: like $100 dollars, not much because the University has an arrangement with SAS but I had to pay that again soon, and I have to pay a lot of money to use it without the University help. Second, is slow as hell, I was going mad using that piece of $%#\\&#94; :-P. And don't get me wrong, the software is \"good\", but is not for me. That is the reason why during this time I learn the basics of R (from coursera ) and python data analysis packages, mainly pandas. The objective on this case is to use Python to replace the functionality I learned this semester of SAS Enterprise Miner . I already knew basic the basics of pandas, numpy and scipy but I still had on my ToDo list learn Scikit-learn . What I wanted to do I wanted to replace the functionality I learned this semester from SAS EM, that is, the very basics: Import data Basic data transformation Model training Model comparison Model scoring So I needed to do the same with iPython, Pandas, Scikit-learn and Matplotlib. On SAS EM a basic project looks like this: Installation I already had installed iPython, pandas and numpy but not Scikit-learn, and since I am using Python 3 (3.2.3) I had to compile scikit-learn from the last development source code: as easy as download the last version from Github and ran python setup.py install Example The data I used is the same I used for a project on the Business Intelligence class, so I could compare the results I obtained from SAS. In a tweet the project is: The data is from people who order products from a catalog, and the idea was to help the company to target the customers who receive the catalogs in order to reduce expenses. There are two CSV files of 2000 records each, one for training and one for testing. The available data is: NGIF: number of orders in the 24 months RAMN: Total order amounts in dollars in the last 24 months LASG: Amount of last order LASD: Date of last order (you may need to convert it to number of months elapsed to 01/01/2007) RFA1: Frequency of order 1=One order in the last 24 months 2=Two orders in the last 24 months 3=Three orders in the last 24 months 4=Four or more orders in the last 24 months RFA2: Order amount category (as defined by the firm) of the last order 1=$0.01 - $1.99 2=$2.00 - $2.99 3=$3.00 e - $4.99 4=$5.00 - $9.99 5=$10.00 - $14.99 6=$15.00 - $24.99 7=$25.00 and above Money: money amount eventually a customer will spend on next order (if she places orders), only available for \"testing\". Order: Actual response (1: response, 0: no response) The file looks like this: CustomerID,NGIF,RAMN,LASG,LASD,RFA1,RFA2,Order 1,2,30,20,200503,1,6,1 2,25,207,20,200503,1,6,0 3,5,52,15,200503,1,6,0 4,11,105,15,200503,1,6,0 5,2,32,17,200503,1,6,0 ... The good part is that the data is clean, everything is a number and there are no missing values. This is far from real, but for a first example is perfect. 1. Import data As simple as: train = pd . read_csv ( 'training.csv' ) test = pd . read_csv ( 'testing.csv' ) train = train . set_index ( 'CustomerID' ) test = test . set_index ( 'CustomerID' ) 2. Data transform Is necesary to convert the LASG (Date of last order) to a normal number; I decide to use MSLO (number of Months Since Last Order). Knowing that the data is from February, 2007 the equation to convert I use is: MSLO = 12*(2007 - YEAR) - MONTH + 2 . On python it translates to: MSLO = train [ 'LASD' ] f = lambda x : 12 * ( 2007 - int ( str ( x )[ 0 : 4 ])) - int ( str ( x )[ 4 : 6 ]) + 2 MSLO = MSLO . apply ( f ) Then just need to remove the LASD column and add the MSLO data: del train [ 'LASD' ] train [ 'MSLO' ] = MSLO 3. Model Training This is covered by scikit-learn, they have tons of models, from Decision Tree to a lot of models, seriously. For this example I use the always good Decision Tree also Support Vector Machine and Naive Bayes. X_train = train [[ 'NGIF' , 'RAMN' , 'LASG' , 'MSLO' , 'RFA1' , 'RFA2' ]] . values Y_train = train [ 'Order' ] . values svm = SVC () svm . probability = True svm . fit ( X_train , Y_train ) tree = DecisionTreeClassifier ( max_depth = 6 ) tree . fit ( X_train , Y_train ) out = export_graphviz ( tree , out_file = 'tree.dot' ) gnb = GaussianNB () gnb . fit ( X_train , Y_train ) 4. Model Scoring/Comparison Scikit-learn comes with Confusion Matrix and ROC Chart included: tree_score = tree . predict ( X_test ) svm_score = svm . predict ( X_test ) gnb_score = gnb . predict ( X_test ) tree_cm = confusion_matrix ( Y_test , tree_score ) svm_cm = confusion_matrix ( Y_test , svm_score ) gnb_cm = confusion_matrix ( Y_test , gnb_score ) # tree array ([[ 1365 , 67 ], [ 506 , 62 ]]) # svm array ([[ 1362 , 70 ], [ 531 , 37 ]]) #gnb array ([[ 1196 , 236 ], [ 406 , 162 ]]) tree_probas = tree . predict_proba ( X_test ) svm_probas = svm . predict_proba ( X_test ) gnb_probas = gnb . predict_proba ( X_test ) tree_fpr , tree_tpr , tree_thresholds = roc_curve ( Y_test , tree_probas [:, 1 ]) svm_fpr , svm_tpr , svm_thresholds = roc_curve ( Y_test , svm_probas [:, 1 ]) gnb_fpr , gnb_tpr , gnb_thresholds = roc_curve ( Y_test , gnb_probas [:, 1 ]) pl . plot ( tree_fpr , tree_tpr , label = 'Tree' ) pl . plot ( svm_fpr , svm_tpr , label = 'SVM' ) pl . plot ( gnb_fpr , gnb_tpr , label = 'GNB' ) pl . xlabel ( 'False Positive Rate' ) pl . ylabel ( 'True Positive Rate' ) pl . title ( 'Receiver operating characteristic' ) pl . legend ( loc = \"lower right\" ) Conclusion With this quick example I was able to get most of the results I learned on my BI class. The analysis of the results is more important but not the objective of this post. Enterprise Miner produces a lot of results but they are useless unless you understand them, is better to get few results and use understand them correctly. Other chart I learned was the Lift chart but scikit-learn does not have this option. I was able to get the results a lot quicker with Python than with SAS EM, for this project it took me like 4 or 5 hours with SAS EM and less than 1 hour on Python. The complete iPython notebook (also visible here ) and data is available on Github: Data Analysis Examples Python - Catalog Marketing . Note : I know this is the most very basic example of Machine Learning on Python, more complex examples will come later.","tags":"01","title":"Python to replace SAS Enterprise Miner (basic)"},{"url":"/blog/python-finance-package-v0-035-event-study-and-market-simulator-integration.html","text":"Ok, here come the first \"Real Life\" example. As I mention on previous posts my finance knowledge is mostly null so I cannot say this can be used on real life but what I found playing with the utils I have created is very interesting. If anyone use this techniques and win/lose money I would love to hear :P Also if anyone find discrepancies on the results please tell me. Changes since last time Bug Fixes on Multiple Event Study: Missing values are now filled: forward and then backwards. Multiple Event Studies: Graph with error bars (std) Integration between the Event Study Finder and Market Simulator Example Use the S&P 500 symbols from 2012 Look in all 2011 for events when the value of the stock went below $5 Discover how to use this opportunity to invest on the equities Simulate if we would make some money from that 1. Setup In [1]: Imports import os from datetime import datetime from finance.utils import BasicUtils from finance.utils import FileManager from finance.utils import ListUtils from finance.evtstudy import EventFinder from finance.evtstudy import MultipleEvents from finance.sim import MarketSimulator In [2]: Download all the information on 2011 of all S&P 500 equities on 2012 SP500_2012 = ListUtils . SP500 ( year = 2012 ) fm = FileManager () start = datetime ( 2011 , 1 , 1 ) end = datetime ( 2011 , 12 , 31 ) files = fm . get_data ( SP500_2012 , start , end ) len ( files ) Out [2]: 498 2. Event: Went Below 5 - One event per Equity In [5]: Look for events evtf = EventFinder () evtf . symbols = SP500_2012 evtf . start_date = start evtf . end_date = end evtf . function = evtf . went_below ( 5 ) evtf . search () In [6]: evtf . num_events Out [6]: 5 Events where found 5 In [7]: Analyse the 5 Events mevt = MultipleEvents () mevt . list = evtf . list mevt . market = 'SPY' mevt . lookback_days = 20 mevt . lookforward_days = 20 mevt . estimation_period = 250 mevt . run () In [8]: Graph the mean cumulative abnornal return mevt . mean_cumulative_abnormal_return . plot () The graph tell us that in average after the event (day 0) the return goes up, but what about the risk? In [9]: Print the Cumulative Abnormal Return with Error Bars mevt . plot ( 'car' ) Ok, the graph tell us that the risk is going up as the time passes, this was expected. Since we are looking the data in 2011 we can see what happen in reality. We use the market simulator but we create the orders from the event list. The instructions are: On the event day: Buy 100 shares 20 days after the event: Sell the 100 Shares Initial cash = $1000 In [10]: Use the market simulator sim = MarketSimulator ( './data' ) sim . field = & quot ; Adj Close & quot ; sim . initial_cash = 1000 sim . create_trades_from_event ( evtf . list , eventDayAction = 'Buy' , eventDayShares = 100 , actionAfter = 'Sell' , daysAfter = 20 , sharesAfter = 100 ) sim . simulate () sim . trades Out [10]: This are the trades generated by the Simulator symbol action num_of_shares 2011-08-08 HBAN Buy 100 2011-09-06 HBAN Sell 100 2011-09-22 GNW Buy 100 2011-10-03 AMD Buy 100 2011-10-20 GNW Sell 100 2011-10-31 AMD Sell 100 2011-11-23 HCBK Buy 100 2011-12-19 BAC Buy 100 2011-12-22 HCBK Sell 100 2012-01-19 BAC Sell 100 In [11]: Graph of the portfolio value: sim . portfolio . plot () BasicUtils . total_return ( sim . portfolio ) Out [11]: 0.501 I obtain 5 events and a total return of 0.501 with just 10 trades. 3. Event: Went below 5 - more than one event per equity From what I saw on the events when one event occurred more events occurred soon (less than 20 days later), that didn't look good for me that's why I tried first with no repeating events. But from my computational investing course I notice that they use all the events so I tried that to: The code is the same just change this on the event finder: evtf . search ( oneEventPerEquity = False ) On this case I found 21 events with a total return of: 0.684; chart: Conlusion Also if anyone find discrepancies on the results please tell me. First, I obtain a total return of 0.501 with just 10 trades; that means that the final value of the portfolio was $1,501. When I saw this I was amazed, just buying shares of very cheap stocks (100 stocks @ <$5 = 500) and then selling those stocks 20 days later was possible to make $501 in just 6 months. Even more if use the second event study. Second, the utils works very well together and with a considerable amount of data, not just 2 equities as before. Third, I am rich! weee! Ok no, not really. But the results are very interesting, at least I was amazed, I never believe of finding a return greater than 0.5. The code + the complete example is on GitHub: PythonFinance This is far from the end but the computational investing course ends with this example so I have no longer a source of inspiration, I am searching for book on quantitative finance to keep learning and developing the package.","tags":"12","title":"Python Finance Package v0.035 - Event study and market simulator Integration"},{"url":"/blog/python-finance-package-v0-03-date-utils-lists-utils-event-studies.html","text":"Continuing with the Python Finance Package . This update took longer because I change my setup to Linux ( elementaryos ), also things are getting complicated/interesting as my finance knowledge is mostly null, so I am learning as I develop the package, that was part of the plan so no complains about that. The objective was to solve the Computational Investing Homework 4. Using the current code on the package (EventFinder + MarketSimulator) is possible to solve the homework, but I will cover that on a later post. Changes since last time Bug fixes on File Manager and Data Access Improved tests Added Date Utils and List Utils to manage the open dates from NYSE Added Event Studies (Single) Past Event Event Finder Multiple Events Date Utils Returns the dates the market was open between two dates. Looks for the closer date on a list of dates and returns the index of that date on the list def nyse_dates ( start = datetime ( 2000 , 1 , 1 ), end = datetime . today (), insideSearch = True , list = False , lookbackDays = 0 , lookforwardDays = 0 ) def search_closer_date ( date , dates , exact = False , searchBack = True , maxDistance = 10 ) Example: from datetime import datetime from finance.utils import DateUtils all_dates = DateUtils . nyse_dates ( list = True ) print ( all_dates ) print ( DateUtils . nyse_dates ( start = datetime ( 2008 , 1 , 1 ))) index = DateUtils . search_closer_date ( datetime ( 2009 , 1 , 1 ), all_dates ) print ( index , all_dates [ index ]) Output: [datetime.datetime(2000, 1, 3, 0, 0), datetime.datetime(2000, 1, 4, 0, 0), datetime.datetime(2000, 1, 5, 0, 0), ... datetime.datetime(2012, 12, 21, 0, 0), datetime.datetime(2012, 12, 24, 0, 0), datetime.datetime(2012, 12, 26, 0, 0)] # Note: Length: 3268 2008-01-02 2008-01-02 00:00:00 2008-01-03 2008-01-03 00:00:00 2008-01-04 2008-01-04 00:00:00 2008-01-07 2008-01-07 00:00:00 ... 2012-12-20 2012-12-20 00:00:00 2012-12-21 2012-12-21 00:00:00 2012-12-24 2012-12-24 00:00:00 2012-12-26 2012-12-26 00:00:00 Length: 1258 2262 2008-12-31 00:00:00 # Note 2009-1-1 was not open, closer date looking back is 2008-12-31 List Utils Date Utils extends the List Utils of dates which only returns all the NYSE dates List Utils also provides the symbols from S&P500 each year Example: from finance.utils import ListUtils sp500_2012 = ListUtils . SP500 ( year = 2012 ) print ( len ( sp500_2012 ), sp500_2012 ) Output: 501 ['A', 'AA', 'AAPL', ... ,'YUM', 'ZION', 'ZMH'] Simple Past Event Input: date of interest, equity to analyze, market symbol to compare, event window length (back and forward) and estimation period length. Output: The program downloads the necessary information and analyse the event: expected return, abnormal return, cumulative abnormal return, t-test. Example (same from: http://www.youtube.com/watch?v=FRNabkJ48vs ): from datetime import datetime from finance.evtstudy import PastEvent pevt = PastEvent ( './data' ) pevt . symbol = 'AAPL' pevt . market = \"\\&#94;gspc\" pevt . lookback_days = 10 pevt . lookforward_days = 10 pevt . estimation_period = 252 pevt . date = datetime ( 2009 , 1 , 5 ) pevt . run () # print(pevt.expected_return) # print(pevt.abnormal_return) print ( pevt . cumulative_abnormal_return ) # print(pevt.t_test) import matplotlib matplotlib . use ( 'Qt4Agg' ) # Probably most people dont need this line import matplotlib.pyplot as plt pevt . expected_return . plot () plt . show () Output: Date 2008-12-18 0.024165 2008-12-19 0.028294 2008-12-22 -0.000580 2008-12-23 0.016884 ... 2009-01-14 0.038178 2009-01-15 0.014626 2009-01-16 -0.004774 2009-01-20 -0.002890 Name: Cumulative Abnormal Return # NOTE: Length: 21 Event Finder Input: List of equities, date range to search for events, event_function [defaults: went_blow(amount), went_above(amount), increase(amount), decrease(amount)] Output: Looks each equity on each date and creates an event matrix Optional/Defaults: Loads/saves a cache (pickle) version of the data. By default only takes the first event on each equity Example: evtf = EventFinder ( './data' ) evtf . symbols = [ 'AMD' , 'CBG' ] evtf . start_date = datetime ( 2008 , 1 , 1 ) evtf . end_date = datetime ( 2010 , 12 , 31 ) evtf . function = evtf . went_below ( 3 ) evtf . search () print ( evtf . num_events ) print ( evtf . matrix ) Output: 2 AMD CBG Date 2008-10-27 1 NaN 2009-02-27 NaN 1 Multiple Events Input: Uses the matrix created on the Event Finder and analyse each event Output: For each event Expected return Abnormal return Cumulative abnormal return Mean of: Expected returns Abnormal returns Cumulative abnormal returns Example: from datetime import datetime from finance.evtstudy import EventFinder , MultipleEvents evtf = EventFinder ( './data' ) evtf . symbols = [ 'AMD' , 'CBG' ] evtf . start_date = datetime ( 2008 , 1 , 1 ) evtf . end_date = datetime ( 2009 , 12 , 31 ) evtf . function = evtf . went_below ( 3 ) evtf . search () mevt = MultipleEvents ( './data' ) mevt . matrix = evtf . matrix mevt . market = 'SPY' mevt . lookback_days = 20 mevt . lookforward_days = 20 mevt . estimation_period = 200 mevt . run () print ( mevt . mean_abnormal_return ) import matplotlib matplotlib . use ( 'Qt4Agg' ) # Probably most people dont need this line import matplotlib.pyplot as plt mevt . mean_cumulative_abnormal_return . plot () plt . show () Output: -20 -0.064385 -19 0.073792 ... -1 -0.011918 0 0.009591 1 -0.119249 ... 19 -0.076149 20 -0.097525 Conclusion As usual the code is on github: PythonFinance What is next? A more real-life example using ipython","tags":"12","title":"Python Finance Package v0.03 – Date Utils, Lists Utils, Event Studies"},{"url":"/blog/python-finance-package-v0-02-basic-utils-and-market-simulator.html","text":"Continuing with the (Basic) Python Finance Package. Now has some basic utils: total-return, daily-returns, and sharpe-ratio calculations. Also a Market Simulator, the purpose was to solve the Computational Investing Homework 3. Changes on DataAccess Now the index of the DataFrame is DatetimeIndex not string. With this is possible to index it with datetimes. Moved the classes folder from './data/' to './utils/' Improved Documentation Market Simulator Reads a .csv file with the orders/trades. Downloads the necessary data (symbols and dates between the ). Makes a simulation of the orders: The .csv file looks like this: year,month,day,symbol,action,num_of_shares 2011,1,10,AAPL,Buy,1500 2011,1,13,AAPL,Sell,1500 2011,1,13,IBM,Buy,4000 2011,1,26,GOOG,Buy,1000 2011,2,2,XOM,Sell,4000 2011,2,10,XOM,Buy,4000 2011,3,3,GOOG,Sell,1000 2011,3,3,IBM,Sell,2200 2011,6,3,IBM,Sell,3300 2011,5,3,IBM,Buy,1500 2011,6,10,AAPL,Buy,1200 2011,8,1,GOOG,Buy,55 2011,8,1,GOOG,Sell,55 2011,12,20,AAPL,Sell,1200 How to use it: Set the initial amount of cash Run the simulation, as an argument give the path to the csv file See the the information from finance.sim import MarketSimulator sim = MarketSimulator () sim . initial_cash = 1000000 sim . simulate ( & quot ; MarketSimulator_orders . csv & quot ;) print ( sim . portfolio [ 0 : 10 ]) import matplotlib matplotlib . use ( 'Qt4Agg' ) \\ # Probably most people dont need this line import matplotlib.pyplot as plt sim . portfolio . plot () plt . show () Output: Portfolio Date 2011-01-10 1000000 2011-01-11 998785 2011-01-12 1002940 2011-01-13 1004815 2011-01-14 1009415 2011-01-18 1011935 2011-01-19 1031495 2011-01-20 1031935 2011-01-21 1030775 2011-01-24 1046815 Basic Utils With the portfolio ready we can get some info about it, for example the total return and sharpe ratio. print ( 'Total Return:' , total_return ( sim . portfolio , 'Portfolio' )) print ( sharpe_ratio ( sim . portfolio , extraAnswers = True )) Which prints: Total Return : 0.1338600000000001 { 'std' : 0.0071901402219816928 , 'sharpe_ratio' : 1.1836398092874625 , 'mean' : 0.0005493527495690362 } Conclusion Next step is to implement an Event Profiler to solve Homework 4. Where to find the code? On github: PythonFinance","tags":"12","title":"Python Finance Package v0.02 – Basic Utils and Market Simulator"},{"url":"/blog/python-finance-package-v0-01-data-manager.html","text":"I have learned a lot from my most recent Coursera Course: Computational Investing Part 1 . The language used on the class is Python so I couldn't be happier; we are using a package called QSTK developed by some people at Georgia Tech, who are also responsible for the class. The quality of the package is amazing, has a tons of features, but one area I notice the package to has to be improved is the data downloading and management. I decide to write some python scripts to help me with this problem, and later on I decide to write a very simple Python Package with a few utils to help me on this class and future finance projects. A little bit an alternative to QSTK and a little bit a complement for it. At the same time I decide to make the jump from Python 2.7 to Python 3.2, this makes QSTK useless for now. The transition from 2.7 to 3.2 was very smooth, I still write print var instead of print(var) a lot of times but is a minor issue. For now the package only has the Data Management part but yesterday I finished my final exams so I have a little bit of time to work on this. How it works You tell the symbol/symbols, dates and the fields (columns) you want from the stocks. The package automatically downloads the information from Yahoo! Finance and loads the information into a Pandas DataFrame. Before downloading the package checks if the information is already downloaded looking into already downloaded information, and optional (default True) saves a pickled version of the DataFrame to load faster the next time. Example from datetime import datetime from finance.data import DataAccess da = DataAccess ( \"./data/\" ) symbols = [ \"AAPL\" , \"GLD\" , \"GOOG\" , \"SPY\" , \"XOM\" ] start_date = datetime ( 2008 , 1 , 1 ) end_date = datetime ( 2009 , 12 , 31 ) fields = \"Close\" close = da . get_data ( symbols , start_date , end_date , fields , save = False ) print ( close ) Little Benchmark Just using clock() and time() to see if it was worth it. It is. Directory empty: Download and save 5 stocks 1.4336301090943933 1.434000015258789 Load 5 stocks from .csv 0.023402424167761726 0.023000001907348633 Load 5 stocks from serialized 0.0007370202310554852 0.0009999275207519531 Where to find the code On github: PythonFinance . This is such a small package is necessary to manually download it and put it on a folder where you have other python packages.","tags":"12","title":"Python Finance Package v0.01 - Data Manager"},{"url":"/blog/using-cloud9-to-save-computing-power-and-to-code-from-everywhere-too.html","text":"I am taking my second course on Coursera Computational Investing - Part 1; pretty fun so far. The homework 1 was to find a portfolio of 4 equities with the best (highest) Sharpe ratio from 2011. With a little bit of coding in python I was able to download all SPY stocks and loop through out the different combinations of them with different weights to find the best portfolio. The problem I found was that my PC is to slow for this task :( First I have to said I am not an algorithm expert so the process is linear, but I was able to optimize a little bit the process by loading into memory first all the data and some other little stuff that makes the calculation faster; that is from 1 1/2 minutes initially to 30 seconds combining 5 stocks. And I was able to combine up to 8 stocks on my PC without any problem but when I try to combine more than 8 well it was so much for my PC; I even take a shower and the calculation wasn't over, that is my limit :P There are solutions to make this type of calculation on the cloud such as PiCloud a service that I am definitely going to try out soon but I found cloud9 was a solution to this problem too. Cloud 9 says they are the Google Docs for code and they really are. Initially I had my doubts about the online editor mainly because I love Sublime text , and yes it is not so powerful as sublime but is very good. But the features such the Github integration and the online terminal sold me the service, I test it and didn't disappoint me. I was able with one-click to clone the repository on my Github and run the code on their infrastructure (powered by OpenShift ) with no problems at all. Just need to install the required libraries ( numpy - running easy_install numpy ) and then it was as simple as run python portfolio.py and see the code running much faster than on my PC. Also the ability to run the code and edit it from everywhere is amazing. I was able to connect from my University and keep testing better portfolios. On the image below I ran 83 iteration of the code in less than 3 seconds, on my PC takes 20 seconds or more. The max number of iterations I ran was up to 250k taking up to 8 minutes. And after modifying the code I was able to push from Cloud9 directly to my Github and back to my PC. Git makes the flow is very smooth and the terminal from Cloud9 good enough for that and more. The service has some problems, sometimes the terminal goes crazy for no reason (that I could find) and I had to refresh the page, but in general works great. I found this a very easy way to execute code on the cloud saving computing power and getting faster responses. Next I want to try PiCloud and OpenShift directly. The code for the computational investing is on my Github if anyone is interested, is going to be updated as the course goes through.","tags":"11","title":"Using cloud9 to save computing power and to code from everywhere too"},{"url":"/blog/making-your-own-ddns-with-django-and-appfog.html","text":"I love web-apps as the data is always in sync magically via the cloud, but sometimes I just need to access my computer via remote access to use some app, get some file or just to start downloading some torrents(shh). Remember/Email/Evernote a changing WAN IP is tedious that is were a DDNS comes in. My previous and old Belkin router had a bunch of options for DDNS s fortunately one was free; unfortunately and unbelievable my new Belkin N750 DB router has only one option: Dyn which is not free :( So I decide to make my own DDNS with Django and with a free PaaS . The first decision was to select a PaaS and there are a lot of option currently; the list got narrowed because I needed a free PaaS for such a simple and small app that it is just for me. Some options were: OpenShift from Red Hat, dotCloud which looks pretty good but not so simple to use and never understand if is really free or not, and other thousand options that are paid and free. I choose AppFog because I recently read that they acquire Nodster (which I loved) and the service is so simple to use, just what I needed for this project. The project consist on two parts; The first one is a Django server which stores previous IPs and acts as the DDNS domain and the second one is a python script that get the WAN IP and request the server to store it. 1. Django Server on AppFog Create the app on AppFog (2 clicks via its web app), download the code via its CLI ( help ) and create a new Django app called ddns . NOTE : AppFog currently runs Django 1.3.1 (current version is 1.4) be sure to look at the correct tutorial/docs on their site. 1.1 Create Django users on AppFog Since it is not possible to do an SSH connection on AppFog (to run commands from the terminal to create the users to access the admin web interface; wow 3 to's on a single phrase that needs to be some kind of record) the simplest solution I could find was to create the with a view and then access it with a URL to create the user, and then change the password ;) def createuser ( request ): if authenticate ( username = 'daniel' , password = 'pass' ) is not None : return HttpResponse ( \"User already created ;)\" ) try : user = User . objects . create_user ( 'daniel' , 'daniel@ctrl68.com' , 'pass' ) user . is_staff = True user . is_superuser = True user . save () return HttpResponse ( \"User created! :)\" ) except : print sys . exc_info ()[ 0 ] return HttpResponse ( \"Couldnt \\\\ ' create user :(\" ) 1.2 Models Just one model to store the IPs by date: class IP ( models . Model ): IP = models . CharField ( max_length = 15 ) date = models . DateTimeField ( 'date' ) def __unicode__ ( self ): return u' %s @ %s ' % ( self . IP , self . date . ctime ()) class Meta : get_latest_by = \"date\" 1.3 Other Views index which does nothing, one to add new IP addresses available via POST or GET requests, and one to redirect to the latest IP. def index ( request ): template = loader . get_template ( 'index.html' ) context = Context ({ 'var' : 0 , }) return HttpResponse ( template . render ( context )) def addip ( request ): ip_txt = \"\" if request . method == 'GET' : ip_txt = request . GET . get ( 'ip' ) elif request . method == 'POST' : ip_txt = request . POST . get ( 'ip' ) try : if ip_txt != \"\" and ip_txt != None : ip = IP ( IP = ip_txt , date = datetime . datetime . now ()) ip . save () ans = \"IP %s added.\" % ip_txt return HttpResponse ( ans ) else : return HttpResponse ( \"No IP.\" ) except : return HttpResponse ( \"Couldnt add %s to db.\" % ip_txt ) def redirect ( request ): ip = IP . objects . latest ( 'date' ) return HttpResponseRedirect ( str ( \"http:// %s \" % ip . IP )) 1.4 Fix Django Admin on AppFog The Django Admin on AppFog doesn't load the CSS from static files correctly I found a solution that is probably not the best but for such a small project, well it is OK. On the main urls.py: from django.contrib.staticfiles.urls import staticfiles_urlpatterns urlpatterns += staticfiles_urlpatterns () 2. Python Script The other part is a python script which reads this page; a nice page that the people of whatismyip.com create to help automation processes like this one. It makes a GET request to the Django app and also saves a file on Dropbox (before this I was using Dropbox as a temporally solution). All inside a while True with a sleep of 10 minutes. Conclusion Thats it! Now I can access my PC from daniel-pc.aws.af.cm - no DDoS please :P I want to look more other PaaS solutions, mainly OpenShift, more coming soon. Complete code is on github .","tags":"10","title":"Making your own DDNS with Django and AppFog"},{"url":"/blog/desktop-app-to-easily-create-new-tasks.html","text":"Recently I have been looking for new ways of manage my tasks, looking for being more productive and organized with my master, coursera courses, blog and programming tasks. I am a huge fan of web apps and the past week I tried almost every (decent = nice landing page) app on the internet, just to name a few: do , doit , producteev , orchestra , wunderlist , asana , 42tasks to finally choose nirvana . I selected nirvana because it was the best GTD app I could find and the UI (very important to me) is pretty good. I haven't read a lot about GTD, mostly a few videos, but the methodology have been working really great for me and I have notice a big boost to my organization, before I was using only Google Calendar. The inbox, next, waiting, schedule, someday and projects lists are a very good way of organize tasks. Even though I love web apps because I can access them from everywhere one big disadvantage of a GTD web app is the task creation process: type the URL, wait for the app to load and then write the new task. The solution most web GTD apps have done is to allow the creation of tasks via email this is good but it is not enough if want to really follow the GTD approach. The task creation need to be really fast, collect first and then organize. Having the app on one tab all the time to gain some seconds is not a solution. I wanted to use GTD the way it is supposed to so I create a little app to create task fast via email, so it works on nivana and any other major task management app that have the option of create tasks via email. I am a web (HTML5, JS, Django, etc...) fan and try to use web technologies in most on my projects but the web is not there yet and a native solution was necessary. I am a python fan, but not even close to an expert and before this little project never use it to create a desktop application with UI. It was really fun to make a desktop app again, almost 3 years have passed since my time with Java in college. The UI library I select was wxPython just because it makes sense with what I already knew from java: the use of grids, vertical and horizontal boxes... also a pretty decent documentation, support and fits the requirements I had for the app. Honorable mention to kivy that looks good but couldn't make it work. The app is pretty simple: just a mailer application. Enter the username (gmail), password and email to send and start sending tasks to your favorite web task management app. As I said the speed was the most important requirement on this app, so it uses a global hot-key (WIN+DEL, just because it works nice on my keyboard) to summon the application then only needs to write the new task, press enter and the app hides on the notification area with a pretty nice icon I get from somewhere. If no longer wants to create the task press esc and it hides waiting to be called. Other stuff I learned from making this app was to encrypt with AES for the password and to create a .exe via py2exe . Cannot compile to other operating systems because I just don't have them. Programmers can download the source can run the app ;) This is a very simple app but it has been working for me and probably will work for other person. Suggestions are always welcome. The code is on github and a zip file with the compiled .exe is available.","tags":"10","title":"Desktop app to easily create new tasks on all task management web apps"},{"url":"/blog/creating-a-blog-in-2012.html","text":"Yes, I am creating a blog on October 5, 2012! Why? Having a flu is one of the reasons. Other reason is that I and want to improve my English and have a lot of time to waste (not having a car on the US is hard). Also having a blog in 2012 is old-stylish and that is always cool. About me: Originally from Colombia and currently living in Frisco, TX (DFW Area) pursuing a Master in Information Technology and Management on the University of Texas at Dallas. Dog lover, book reader, TV Series Fanatic and amateur gamer. Passionate about new technologies, always wanting to learn something new (mainly about technology). Hopes to keep writing for a long time. -Dan","tags":"10","title":"Creating a blog in 2012"}]}